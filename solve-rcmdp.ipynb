{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[CpuDevice(id=0),\n",
       " CpuDevice(id=1),\n",
       " CpuDevice(id=2),\n",
       " CpuDevice(id=3),\n",
       " CpuDevice(id=4),\n",
       " CpuDevice(id=5),\n",
       " CpuDevice(id=6),\n",
       " CpuDevice(id=7),\n",
       " CpuDevice(id=8),\n",
       " CpuDevice(id=9),\n",
       " CpuDevice(id=10),\n",
       " CpuDevice(id=11),\n",
       " CpuDevice(id=12),\n",
       " CpuDevice(id=13),\n",
       " CpuDevice(id=14),\n",
       " CpuDevice(id=15),\n",
       " CpuDevice(id=16),\n",
       " CpuDevice(id=17),\n",
       " CpuDevice(id=18),\n",
       " CpuDevice(id=19)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "NUM_DEVICES = 20\n",
    "os.environ[\"XLA_FLAGS\"] = f'--xla_force_host_platform_device_count={NUM_DEVICES}'\n",
    "\n",
    "import jax\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test passed\n"
     ]
    }
   ],
   "source": [
    "from finite.rcmdp import RCMDP, compute_policy_Q, compute_policy_worst_values\n",
    "from finite.garnet import create_rcmdp, S, A, N, DISCOUNT, ITER_LENGTH, NUM_SEEDS, FIGNAME\n",
    "# from finite.streaming import create_rcmdp, S, A, N, DISCOUNT, ITER_LENGTH, NUM_SEEDS, FIGNAME\n",
    "\n",
    "assert NUM_DEVICES >= NUM_SEEDS, \"NUM_DEVICES must be greater than NUM_SEEDS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "@jax.jit\n",
    "def projection_to_simplex(y):\n",
    "    \"\"\"project y to a probability simplex\n",
    "    see：https://arxiv.org/pdf/1309.1541\n",
    "    Args:\n",
    "        y (jnp.ndarray): (A)-vector\n",
    "\n",
    "    Returns:\n",
    "        x (jnp.ndarray): (A)-vector\n",
    "    \"\"\"\n",
    "    D = len(y)\n",
    "    u = jnp.sort(y)[::-1]\n",
    "    u_sum = jnp.cumsum(u)\n",
    "    rho_pos_flag = (u + (1 - u_sum) / (jnp.arange(D) + 1)) > 0\n",
    "    rho = jnp.argmax(jnp.cumsum(rho_pos_flag))\n",
    "    lam = (1 - u_sum[rho]) / (rho + 1)\n",
    "    x = jnp.maximum(y + lam, 0)\n",
    "    return x\n",
    "\n",
    "\n",
    "proj_to_Pi = jax.vmap(projection_to_simplex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import chex\n",
    "\n",
    "\n",
    "policy = jnp.ones((NUM_SEEDS, S, A)) / A\n",
    "sum_policy = jnp.zeros((NUM_SEEDS, S, A))\n",
    "lam = jnp.zeros((NUM_SEEDS, N))\n",
    "res_J0U_list = jnp.zeros((NUM_SEEDS, ITER_LENGTH))\n",
    "vios_list = jnp.zeros((NUM_SEEDS, ITER_LENGTH))\n",
    "res_J0U_list = jnp.zeros((NUM_SEEDS, ITER_LENGTH))\n",
    "vios_list = jnp.zeros((NUM_SEEDS, ITER_LENGTH))\n",
    "res_J0U_avg_list = jnp.zeros((NUM_SEEDS, ITER_LENGTH))\n",
    "vios_avg_list = jnp.zeros((NUM_SEEDS, ITER_LENGTH))\n",
    "\n",
    "InitLagArgs = res_J0U_list, vios_list, res_J0U_avg_list, vios_avg_list, policy, sum_policy, lam \n",
    "\n",
    "@jax.jit\n",
    "def solve_inner_Lagrange(lam: float, rcmdp: RCMDP, init_policy: jnp.ndarray, num_iter: int, lr: float):\n",
    "    \"\"\"Apply policy gradients to the inner minimization problem of the Lagrangian formulation.\n",
    "    See Algorithm 3 in the paper.\n",
    "\n",
    "    Args:\n",
    "        lam (float): Lagrangian variable\n",
    "        rcmdp (RCMDP)\n",
    "        init_policy (jnp.ndarray): Initial policy\n",
    "        num_iter (int): Number of iteration\n",
    "        lr (float): learning rate to update policy\n",
    "\n",
    "    Returns:\n",
    "        policy (jnp.ndarray): (SxA) array\n",
    "    \"\"\"\n",
    "    chex.assert_shape(lam, (N, ))\n",
    "    one_lam = jnp.hstack([jnp.array([1,]), lam])\n",
    "\n",
    "    def condition_fn(loop_args):\n",
    "        # break iteration if the policy does not change or k >= num_iter\n",
    "        k, _, _, _, policy_diff = loop_args\n",
    "        return (k < num_iter) & (policy_diff > 1e-5)\n",
    "\n",
    "    def loop_fn(loop_args):\n",
    "        k, policy, best_policy, best_L_lam, _ = loop_args\n",
    "\n",
    "        # evaluate current policy\n",
    "        worst_P_Q, worst_P_occ, worst_P_J = compute_policy_worst_values(policy, rcmdp)\n",
    "        L_lam = (one_lam.reshape(N+1) * worst_P_J.reshape(N+1)).sum()\n",
    "        best_policy = jax.lax.cond(L_lam < best_L_lam, lambda: policy, lambda: best_policy)\n",
    "        best_L_lam = jnp.minimum(L_lam, best_L_lam)\n",
    "\n",
    "        grad = jnp.sum(one_lam.reshape(N+1, 1, 1) * worst_P_occ.reshape(N+1, S, 1) * worst_P_Q, axis=0)\n",
    "        new_policy = proj_to_Pi(policy - lr * grad)\n",
    "\n",
    "        policy_diff = jnp.abs(new_policy - policy).sum()\n",
    "        return k+1, new_policy, best_policy, best_L_lam, policy_diff\n",
    "    \n",
    "    best_policy = init_policy\n",
    "    best_L_lam = jnp.inf\n",
    "    # k, new_policy, best_policy, best_L_lam, policy_diff = jax.lax.while_loop(condition_fn, loop_fn, (0, init_policy, best_policy, best_L_lam, jnp.inf))\n",
    "    _, _, best_policy, _, _ = jax.lax.while_loop(condition_fn, loop_fn, (0, init_policy, best_policy, best_L_lam, jnp.inf))\n",
    "    # return k, new_policy, best_policy, best_L_lam, policy_diff\n",
    "    return best_policy\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update_outer_Lagrange(init_args, init_k: int, end_k: int, rcmdp: RCMDP, lam_lr: float = 0.01, inner_iter: int=1000, inner_lr: float = 0.001):\n",
    "    \"\"\"Update Lagrangian variable (end_k - init_k) times.\n",
    "    See Algorithm 3 in the paper.\n",
    "\n",
    "    Args:\n",
    "        init_args: These arguments will be passed to the foriloop of jax. See InitLagArgs defined above.\n",
    "        init_k (int): initial update index\n",
    "        end_k (int): end of update index\n",
    "\n",
    "    Returns:\n",
    "        args: Computed arguments. See InitLagArgs defined above.\n",
    "    \"\"\"\n",
    "\n",
    "    def eval_performance(policy):\n",
    "        _, _, worst_P_J = compute_policy_worst_values(policy, rcmdp)\n",
    "        vio = worst_P_J[1:] - rcmdp.threshes\n",
    "        chex.assert_shape(vio, (N, ))\n",
    "        return worst_P_J, vio\n",
    "\n",
    "    def body_fn(k, args):\n",
    "        res_J0U_list, vios_list, res_J0U_avg_list, vios_avg_list, policy, sum_policy, lam = args\n",
    "        policy = solve_inner_Lagrange(lam, rcmdp, policy, inner_iter, inner_lr)\n",
    "        worst_P_J, vio = eval_performance(policy)\n",
    "\n",
    "        # report performance\n",
    "        res_J0U_list = res_J0U_list.at[k].set(worst_P_J[0])\n",
    "        vios_list = vios_list.at[k].set(vio.max())\n",
    "\n",
    "        # update Lagrange\n",
    "        new_lam = lam + lam_lr * vio\n",
    "        lam = jnp.maximum(new_lam, 0)\n",
    "\n",
    "        # report the averaged policy performance\n",
    "        sum_policy = sum_policy + policy\n",
    "        avg_policy = sum_policy / (k + 1)\n",
    "        worst_P_J, vio = eval_performance(avg_policy)\n",
    "        res_J0U_avg_list = res_J0U_avg_list.at[k].set(worst_P_J[0])\n",
    "        vios_avg_list = vios_avg_list.at[k].set(vio.max())\n",
    "        return res_J0U_list, vios_list, res_J0U_avg_list, vios_avg_list, policy, sum_policy, lam\n",
    "\n",
    "    args = jax.lax.fori_loop(init_k, end_k, body_fn, init_args)\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "policy = jnp.ones((NUM_SEEDS, S, A)) / A\n",
    "i = jnp.zeros((NUM_SEEDS))\n",
    "j = jnp.ones((NUM_SEEDS)) * 1 / (1 - DISCOUNT)\n",
    "res_J0U_list = jnp.zeros((NUM_SEEDS, ITER_LENGTH))\n",
    "vios_list = jnp.zeros((NUM_SEEDS, ITER_LENGTH))\n",
    "InitEFArgs = res_J0U_list, vios_list, policy, i, j\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def solve_inner_EF(b0: float, rcmdp: RCMDP, init_policy: jnp.ndarray, num_iter: int, lr: float):\n",
    "    \"\"\"Apply policy gradients to the auxiliary minimization problem of the epigraph form.\n",
    "    See Algorithm 1 in the paper.\n",
    "\n",
    "    Args:\n",
    "        b0 (float): Threshold variable\n",
    "        rcmdp (RCMDP)\n",
    "        init_policy (jnp.ndarray): Initial policy\n",
    "        num_iter (int): Number of iteration\n",
    "        lr (float): learning rate to update policy\n",
    "\n",
    "    Returns:\n",
    "        policy (jnp.ndarray): (SxA) array\n",
    "    \"\"\"\n",
    "    b0_threshes = jnp.hstack([jnp.array([b0,]), rcmdp.threshes])\n",
    "\n",
    "    def condition_fn(loop_args):\n",
    "        # break iteration if Δ <= 0 or k >= num_iter\n",
    "        k, _, _, best_Delta, policy_diff = loop_args\n",
    "        return (k < num_iter) & (best_Delta > 0) & (policy_diff > 1e-5) # the k'th policy has not been evaluated yet\n",
    "\n",
    "    def loop_fn(loop_args):\n",
    "        k, policy, best_policy, best_Delta, _ = loop_args\n",
    "\n",
    "        # evaluate current policy\n",
    "        worst_P_Q, worst_P_occ, worst_P_J = compute_policy_worst_values(policy, rcmdp)\n",
    "        Delta = jnp.max(worst_P_J - b0_threshes)\n",
    "        best_policy = jax.lax.cond(Delta < best_Delta, lambda: policy, lambda: best_policy)\n",
    "        best_Delta = jnp.minimum(Delta, best_Delta)\n",
    "\n",
    "        # compute gradient\n",
    "        worst_vio_idx = jnp.argmax(worst_P_J - b0_threshes)\n",
    "        worst_Q, worst_occ = worst_P_Q[worst_vio_idx], worst_P_occ[worst_vio_idx]\n",
    "        chex.assert_shape(worst_occ, (S,))\n",
    "        chex.assert_shape(worst_Q, (S, A))\n",
    "        grad = worst_occ.reshape(-1, 1) * worst_Q\n",
    "\n",
    "        # update to new policy\n",
    "        new_policy = proj_to_Pi(policy - lr * grad)\n",
    "        policy_diff = jnp.abs(new_policy - policy).sum()\n",
    "        return k+1, new_policy, best_policy, best_Delta, policy_diff\n",
    "    \n",
    "    best_policy = init_policy\n",
    "    best_Delta = jnp.inf\n",
    "    _, _, best_policy, _, _ = jax.lax.while_loop(condition_fn, loop_fn, (0, init_policy, best_policy, best_Delta, jnp.inf))\n",
    "   \n",
    "    return best_policy\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update_outer_EF(args, init_k: int, end_k: int, rcmdp: RCMDP, inner_iter: int=1000, inner_lr: float = 0.001):\n",
    "    \"\"\"Update the threshold variable (end_k - init_k) times.\n",
    "    See Algorithm 2 in the paper.\n",
    "\n",
    "    Args:\n",
    "        init_args: These arguments will be passed to the foriloop of jax. See InitEFArgs defined above.\n",
    "        init_k (int): initial update index\n",
    "        end_k (int): end of update index\n",
    "\n",
    "    Returns:\n",
    "        args: Computed arguments. See InitEFArgs defined above.\n",
    "    \"\"\"\n",
    "    def body_fn(k, args):\n",
    "        res_J0U_list, vios_list, policy, i, j = args\n",
    "        b0 = (i + j) / 2\n",
    "        policy = solve_inner_EF(b0, rcmdp, policy, inner_iter, inner_lr)\n",
    "        _, _, worst_P_J = compute_policy_worst_values(policy, rcmdp)\n",
    "        b0_threshes = jnp.hstack([jnp.array([b0,]), rcmdp.threshes])\n",
    "        Delta = jnp.max(worst_P_J - b0_threshes)\n",
    "\n",
    "        i = jax.lax.cond(Delta > 0, lambda: b0, lambda: i)\n",
    "        j = jax.lax.cond(Delta <= 0, lambda: b0, lambda: j)\n",
    "\n",
    "        res_J0U_list = res_J0U_list.at[k].set(worst_P_J[0])\n",
    "        vio = worst_P_J[1:] - rcmdp.threshes\n",
    "        vios_list = vios_list.at[k].set(vio.max())\n",
    "        return res_J0U_list, vios_list, policy, i, j\n",
    "\n",
    "    args = jax.lax.fori_loop(init_k, end_k, body_fn, args)\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.pmap, in_axes=(0, None, None, 0, 0))\n",
    "def update_Args(seed, init_k, end_k, LagArgs, EFArgs):\n",
    "    rcmdp = create_rcmdp(seed)\n",
    "    LagArgs = update_outer_Lagrange(LagArgs, init_k, end_k, rcmdp)\n",
    "    EFArgs = update_outer_EF(EFArgs, init_k, end_k, rcmdp)\n",
    "\n",
    "    uniform_policy = jnp.ones((S, A)) / A\n",
    "    Qs = compute_policy_Q(rcmdp.discount, uniform_policy, rcmdp.costs, rcmdp.U)  # N+1 x |U| x S x A\n",
    "    Js = jnp.sum((Qs * uniform_policy.reshape(1, 1, S, A)).sum(axis=-1) * rcmdp.init_dist.reshape(1, 1, S), axis=-1).max(axis=-1)\n",
    "    UJ, Uv = Js[0], jnp.max(Js[1:] - rcmdp.threshes)\n",
    "    return LagArgs, EFArgs, UJ, Uv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [06:18<00:00,  1.89s/it]\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "UNROLL_ITER = 5\n",
    "seeds = jnp.arange(NUM_SEEDS)\n",
    "LagArgs, EFArgs = deepcopy(InitLagArgs), deepcopy(InitEFArgs)\n",
    "for i in tqdm(range(int(ITER_LENGTH / UNROLL_ITER))):\n",
    "    LagArgs, EFArgs, Uni_J0U_list, Uni_vio_list = update_Args(seeds, UNROLL_ITER * i, UNROLL_ITER * (i+1), LagArgs, EFArgs)\n",
    "Lag_J0U_list, Lag_vio_list, Lag_J0U_avg_list, Lag_vio_avg_list, *_ = LagArgs\n",
    "EF_J0U_list, EF_vio_list, *_ = EFArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Uni_J0U_list_rep = jnp.repeat(Uni_J0U_list.reshape(-1, 1), ITER_LENGTH, axis=1)\n",
    "Uni_vio_list_rep = jnp.repeat(Uni_vio_list.reshape(-1, 1), ITER_LENGTH, axis=1)\n",
    "J_baseval = Uni_J0U_list.reshape(-1, 1)\n",
    "\n",
    "Unif, LF, LFavg, EF = r\"Uniform policy ($\\pi_{\\mathrm{unif}}$)\", \"LF-PGS\", \"LF-PGS-avg\", r\"$\\mathbf{EpiRC\\operatorname{-}PGS\\;(Ours)}$\"\n",
    "\n",
    "algos =  {Unif: (Uni_J0U_list_rep - J_baseval, Uni_vio_list_rep),\n",
    "          LF: (Lag_J0U_list - J_baseval, Lag_vio_list), \n",
    "          LFavg: (Lag_J0U_avg_list - J_baseval, Lag_vio_avg_list), \n",
    "          EF: (EF_J0U_list - J_baseval, EF_vio_list), \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"results/{FIGNAME}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(algos, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
