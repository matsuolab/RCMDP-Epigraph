{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The overall experiments will finish about 30 minutes using 20 CPUs\n",
    "import os\n",
    "os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=20'\n",
    "\n",
    "import jax\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "class RCMDP(NamedTuple):\n",
    "    S_set: jnp.array  # state space\n",
    "    A_set: jnp.array  # action space\n",
    "    discount: float  # discount\n",
    "    costs: jnp.array  # cost functions\n",
    "    threshes: jnp.array  # constraint thresholds\n",
    "    U: jnp.array  # uncertainty set\n",
    "    init_dist: jnp.array  # initial distribution\n",
    "\n",
    "    @property\n",
    "    def S(self) -> int:  # state space size\n",
    "        return len(self.S_set)\n",
    "\n",
    "    @property\n",
    "    def A(self) -> int:  # action space size\n",
    "        return len(self.A_set)\n",
    "\n",
    "\n",
    "S, A = 15, 5  # state and action space sizes\n",
    "N = 1  # number of constraints\n",
    "USIZE = 5  # size of uncertainty set\n",
    "DISCOUNT = 0.99 \n",
    "ITER_LENGTH = 1000  # iteration length for experiment\n",
    "NUM_SEEDS = 20  # number of evaluation seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chex\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "@partial(jax.vmap, in_axes=(None, None, 0, None), out_axes=0)\n",
    "@partial(jax.vmap, in_axes=(None, None, None, 0), out_axes=0)\n",
    "def compute_greedy_Q(discount: float, iter: int, cost: jnp.ndarray, P: jnp.ndarray):\n",
    "    \"\"\"Compute a greedy Q function with respect to the constraint cost function in P\n",
    "    Args:\n",
    "        discount (float)\n",
    "        cost (jnp.ndarray)\n",
    "        P (jnp.ndarray)\n",
    "\n",
    "    Returns:\n",
    "        optimal_Q (jnp.ndarray): (SxA)の行列\n",
    "    \"\"\"\n",
    "\n",
    "    def backup(optimal_Q):\n",
    "        next_v = P @ optimal_Q.min(axis=1)\n",
    "        assert next_v.shape == (S, A)\n",
    "        return cost + discount * next_v\n",
    "    \n",
    "    optimal_Q = jnp.zeros((S, A))\n",
    "    body_fn = lambda i, Q: backup(Q)\n",
    "    return jax.lax.fori_loop(0, iter, body_fn, optimal_Q)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def projection_to_simplex(y):\n",
    "    \"\"\"project y to a probability simplex\n",
    "    see：https://arxiv.org/pdf/1309.1541\n",
    "    Args:\n",
    "        y (jnp.ndarray): (A)-vector\n",
    "\n",
    "    Returns:\n",
    "        x (jnp.ndarray): (A)-vector\n",
    "    \"\"\"\n",
    "    D = len(y)\n",
    "    u = jnp.sort(y)[::-1]\n",
    "    u_sum = jnp.cumsum(u)\n",
    "    rho_pos_flag = (u + (1 - u_sum) / (jnp.arange(D) + 1)) > 0\n",
    "    rho = jnp.argmax(jnp.cumsum(rho_pos_flag))\n",
    "    lam = (1 - u_sum[rho]) / (rho + 1)\n",
    "    x = jnp.maximum(y + lam, 0)\n",
    "    return x\n",
    "\n",
    "\n",
    "proj_to_Pi = jax.vmap(projection_to_simplex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import chex\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_matrix(policy: jnp.ndarray):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        policy (jnp.ndarray): (SxA) array\n",
    "\n",
    "    Returns:\n",
    "        policy_matrix (jnp.ndarray): (SxSA) array\n",
    "    \"\"\"\n",
    "    S, A = policy.shape\n",
    "    PI = policy.reshape(1, S, A)\n",
    "    PI = jnp.tile(PI, (S, 1, 1))\n",
    "    eyes = jnp.eye(S).reshape(S, S, 1)\n",
    "    PI = (eyes * PI).reshape(S, S*A)\n",
    "    return PI\n",
    "\n",
    "\n",
    "@partial(jax.vmap, in_axes=(None, None, 0, None), out_axes=0)\n",
    "@partial(jax.vmap, in_axes=(None, None, None, 0), out_axes=0)\n",
    "def compute_policy_Q(discount: float, policy: jnp.ndarray, cost: jnp.ndarray, P: jnp.ndarray):\n",
    "    \"\"\" Do policy evaluation with cost and transition kernel\n",
    "    Args:\n",
    "        discount (float): discount factor\n",
    "        policy (jnp.ndarray): (SxA) array\n",
    "        cost (jnp.ndarray): cost function. (SxA) array\n",
    "        P (jnp.ndarray): transition kernel. (SxAxS) array\n",
    "\n",
    "    Returns:\n",
    "        Q (jnp.ndarray): (SxA) array\n",
    "    \"\"\"\n",
    "    S, A = policy.shape\n",
    "\n",
    "    Pi = compute_policy_matrix(policy)\n",
    "    PPi = P.reshape(S*A, S) @ Pi\n",
    "    Q = jnp.linalg.inv(jnp.eye(S*A) - discount * PPi) @ cost.reshape(S*A)\n",
    "    return Q.reshape(S, A)\n",
    "\n",
    "\n",
    "@partial(jax.vmap, in_axes=(None, None, None, 0), out_axes=0)\n",
    "def compute_policy_visit_s(discount: float, policy: jnp.ndarray, init_dist: jnp.ndarray, P: jnp.ndarray):\n",
    "    \"\"\" Compute (unnormalized) occupancy measure of a policy\n",
    "    Args:\n",
    "        discount (float): discount factor\n",
    "        policy (jnp.ndarray): (SxA) array\n",
    "        init_dist: initial distribution\n",
    "        P (jnp.ndarray): (SxAxS) array\n",
    "\n",
    "    Returns:\n",
    "        d_pi (jnp.ndarray): (S) array\n",
    "    \"\"\"\n",
    "    Pi = compute_policy_matrix(policy)\n",
    "    PiP = Pi @ P.reshape(S*A, S) \n",
    "    d_pi = init_dist @ jnp.linalg.inv(jnp.eye(S) - discount * PiP)\n",
    "    return d_pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Optional\n",
    "from jax.random import PRNGKey\n",
    "\n",
    "\n",
    "def create_rcmdp(seed: int):\n",
    "    key = PRNGKey(seed)\n",
    "\n",
    "    S_set = jnp.arange(S)\n",
    "    A_set = jnp.arange(A)\n",
    "    const = jnp.zeros(N)  # dummy\n",
    "\n",
    "    # randomly create cost function\n",
    "    costs = jnp.ones((N+1, S, A))\n",
    "    key, _key = jax.random.split(key)\n",
    "    zero_mask = jax.random.bernoulli(_key, p=0.5, shape=costs.shape)\n",
    "    costs = costs * zero_mask\n",
    "    costs = costs.at[1].set(1 - costs[0])\n",
    "\n",
    "    # create initial distribution\n",
    "    key, _key = jax.random.split(key)\n",
    "    init_dist = jax.random.dirichlet(key=_key, alpha=jnp.array([0.1] * S))\n",
    "    # np.testing.assert_allclose(init_dist.sum(axis=-1), 1, atol=1e-6)\n",
    "\n",
    "    # create uncertainty set\n",
    "    U = jnp.zeros((USIZE, S, A, S))\n",
    "    for u in range(USIZE):\n",
    "        key, _key = jax.random.split(key)\n",
    "        P = jax.random.dirichlet(key=_key, alpha=jnp.array([0.05] * S), shape=((S*A,)))\n",
    "        P = P.reshape(S, A, S)\n",
    "        # np.testing.assert_allclose(P.sum(axis=-1), 1, atol=1e-6)\n",
    "        U = U.at[u].set(P)\n",
    "\n",
    "    rcmdp = RCMDP(S_set, A_set, DISCOUNT, costs, const, U, init_dist)\n",
    "\n",
    "\n",
    "    # set the constraint threshold based on the possible constraint satisfaction\n",
    "    const = 0\n",
    "    greedy_Qs = compute_greedy_Q(rcmdp.discount, int(1 / (1-rcmdp.discount)) + 100, rcmdp.costs, rcmdp.U)\n",
    "    greedy_idxes = greedy_Qs.argmin(axis=-1).reshape(-1, S)\n",
    "    greedy_policy = jnp.zeros((S, A))\n",
    "    for i in range((N+1) * USIZE):\n",
    "        greedy_policy = greedy_policy.at[jnp.arange(S), greedy_idxes[i]].add(1)\n",
    "    greedy_policy = greedy_policy / ((N+1) * USIZE)\n",
    "    # np.testing.assert_allclose(greedy_policy.sum(axis=-1), 1, atol=1e-6)\n",
    "\n",
    "    threshes = []\n",
    "    Qs = compute_policy_Q(rcmdp.discount, greedy_policy, costs, rcmdp.U)\n",
    "    Vs = (greedy_policy.reshape(1, 1, S, A) * Qs).sum(axis=-1)\n",
    "    Js = (Vs * rcmdp.init_dist.reshape(1, 1, S)).sum(axis=-1)\n",
    "    # assert Js.shape == (N + 1, USIZE)\n",
    "    threshes = Js.max(axis=1)[1:]\n",
    "\n",
    "    rcmdp = rcmdp._replace(threshes=jnp.array(threshes))\n",
    "    return rcmdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import chex\n",
    "\n",
    "\n",
    "policy = jnp.ones((NUM_SEEDS, S, A)) / A\n",
    "sum_policy = jnp.zeros((NUM_SEEDS, S, A))\n",
    "lam = jnp.zeros((NUM_SEEDS, N))\n",
    "res_J0U_list = jnp.zeros((NUM_SEEDS, ITER_LENGTH))\n",
    "vios_list = jnp.zeros((NUM_SEEDS, ITER_LENGTH))\n",
    "res_J0U_list = jnp.zeros((NUM_SEEDS, ITER_LENGTH))\n",
    "vios_list = jnp.zeros((NUM_SEEDS, ITER_LENGTH))\n",
    "res_J0U_avg_list = jnp.zeros((NUM_SEEDS, ITER_LENGTH))\n",
    "vios_avg_list = jnp.zeros((NUM_SEEDS, ITER_LENGTH))\n",
    "\n",
    "InitLagArgs = res_J0U_list, vios_list, res_J0U_avg_list, vios_avg_list, policy, sum_policy, lam \n",
    "\n",
    "@jax.jit\n",
    "def solve_inner_Lagrange(lam: float, rcmdp: RCMDP, init_policy: jnp.ndarray, num_iter: int, lr: float):\n",
    "    \"\"\"Apply policy gradients to the inner minimization problem of the Lagrangian formulation.\n",
    "    See Algorithm 3 in the paper.\n",
    "\n",
    "    Args:\n",
    "        lam (float): Lagrangian variable\n",
    "        rcmdp (RCMDP)\n",
    "        init_policy (jnp.ndarray): Initial policy\n",
    "        num_iter (int): Number of iteration\n",
    "        lr (float): learning rate to update policy\n",
    "\n",
    "    Returns:\n",
    "        policy (jnp.ndarray): (SxA) array\n",
    "    \"\"\"\n",
    "    chex.assert_shape(lam, (N, ))\n",
    "    one_lam = jnp.hstack([jnp.array([1,]), lam])\n",
    "\n",
    "    def loop_fn(k, policy):\n",
    "        Qs = compute_policy_Q(rcmdp.discount, policy, rcmdp.costs, rcmdp.U)  # N+1 x |U| x S x A\n",
    "        Js = jnp.sum((Qs * policy.reshape(1, 1, S, A)).sum(axis=-1) * rcmdp.init_dist.reshape(1, 1, S), axis=-1)\n",
    "        ds = compute_policy_visit_s(rcmdp.discount, policy, rcmdp.init_dist, rcmdp.U)\n",
    "        idx = jnp.argmax(Js, axis=-1)\n",
    "        chex.assert_shape(idx, (N+1, ))\n",
    "        chex.assert_shape(ds, (USIZE, S))\n",
    "\n",
    "        Qs_U = jnp.zeros((N+1, S, A))\n",
    "        for n in range(N+1):\n",
    "            Qs_U = Qs_U.at[n].set(Qs[n, idx[n]])\n",
    "        dsU = ds[idx]\n",
    "        chex.assert_shape(dsU, (N+1, S))\n",
    "\n",
    "        grad = jnp.sum(one_lam.reshape(N+1, 1, 1) * dsU.reshape(N+1, S, 1) * Qs_U, axis=0)\n",
    "        policy = proj_to_Pi(policy - lr * grad)\n",
    "        return policy\n",
    "    \n",
    "    policy = jax.lax.fori_loop(0, num_iter, loop_fn, init_policy)\n",
    "    return policy\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update_outer_Lagrange(init_args, init_k: int, end_k: int, rcmdp: RCMDP, lam_lr: float = 0.01, inner_iter: int=1000, inner_lr: float = 0.001):\n",
    "    \"\"\"Update Lagrangian variable (end_k - init_k) times.\n",
    "    See Algorithm 3 in the paper.\n",
    "\n",
    "    Args:\n",
    "        init_args: These arguments will be passed to the foriloop of jax. See InitLagArgs defined above.\n",
    "        init_k (int): initial update index\n",
    "        end_k (int): end of update index\n",
    "\n",
    "    Returns:\n",
    "        args: Computed arguments. See InitLagArgs defined above.\n",
    "    \"\"\"\n",
    "\n",
    "    def eval_performance(policy):\n",
    "        Qs = compute_policy_Q(rcmdp.discount, policy, rcmdp.costs, rcmdp.U)  # N+1 x |U| x S x A\n",
    "        Js = jnp.sum((Qs * policy.reshape(1, 1, S, A)).sum(axis=-1) * rcmdp.init_dist.reshape(1, 1, S), axis=-1).max(axis=-1)\n",
    "        vio = Js[1:] - rcmdp.threshes\n",
    "        chex.assert_shape(vio, (N, ))\n",
    "        return Js, vio\n",
    "\n",
    "    def body_fn(k, args):\n",
    "        res_J0U_list, vios_list, res_J0U_avg_list, vios_avg_list, policy, sum_policy, lam = args\n",
    "        policy = solve_inner_Lagrange(lam, rcmdp, policy, inner_iter, inner_lr)\n",
    "        Js, vio = eval_performance(policy)\n",
    "\n",
    "        # report performance\n",
    "        res_J0U_list = res_J0U_list.at[k].set(Js[0])\n",
    "        vios_list = vios_list.at[k].set(vio.max())\n",
    "\n",
    "        # update Lagrange\n",
    "        new_lam = lam + lam_lr * vio\n",
    "        lam = jnp.maximum(new_lam, 0)\n",
    "\n",
    "        # report the averaged policy performance\n",
    "        sum_policy = sum_policy + policy\n",
    "        avg_policy = sum_policy / (k + 1)\n",
    "        Js, vio = eval_performance(avg_policy)\n",
    "        res_J0U_avg_list = res_J0U_avg_list.at[k].set(Js[0])\n",
    "        vios_avg_list = vios_avg_list.at[k].set(vio.max())\n",
    "        return res_J0U_list, vios_list, res_J0U_avg_list, vios_avg_list, policy, sum_policy, lam\n",
    "\n",
    "    args = jax.lax.fori_loop(init_k, end_k, body_fn, init_args)\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "policy = jnp.ones((NUM_SEEDS, S, A)) / A\n",
    "i = jnp.zeros((NUM_SEEDS))\n",
    "j = jnp.ones((NUM_SEEDS)) * 1 / (1 - DISCOUNT)\n",
    "res_J0U_list = jnp.zeros((NUM_SEEDS, ITER_LENGTH))\n",
    "vios_list = jnp.zeros((NUM_SEEDS, ITER_LENGTH))\n",
    "InitEFArgs = res_J0U_list, vios_list, policy, i, j\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def solve_inner_EF(b0: float, rcmdp: RCMDP, init_policy: jnp.ndarray, num_iter: int, lr: float):\n",
    "    \"\"\"Apply policy gradients to the auxiliary minimization problem of the epigraph form.\n",
    "    See Algorithm 1 in the paper.\n",
    "\n",
    "    Args:\n",
    "        b0 (float): Threshold variable\n",
    "        rcmdp (RCMDP)\n",
    "        init_policy (jnp.ndarray): Initial policy\n",
    "        num_iter (int): Number of iteration\n",
    "        lr (float): learning rate to update policy\n",
    "\n",
    "    Returns:\n",
    "        policy (jnp.ndarray): (SxA) array\n",
    "    \"\"\"\n",
    "    b0_threshes = jnp.hstack([jnp.array([b0,]), rcmdp.threshes])\n",
    "\n",
    "    def loop_fn(_, policy):\n",
    "        Qs = compute_policy_Q(rcmdp.discount, policy, rcmdp.costs, rcmdp.U)  # N+1 x |U| x S x A\n",
    "        Js = jnp.sum((Qs * policy.reshape(1, 1, S, A)).sum(axis=-1) * rcmdp.init_dist.reshape(1, 1, S), axis=-1)\n",
    "        ds = compute_policy_visit_s(rcmdp.discount, policy, rcmdp.init_dist, rcmdp.U)\n",
    "        chex.assert_shape(Js, (N+1, USIZE))\n",
    "        idx = jnp.argmax(Js, axis=-1)\n",
    "        chex.assert_shape(idx, (N+1, ))\n",
    "        chex.assert_shape(ds, (USIZE, S))\n",
    "\n",
    "        Qs_U = jnp.zeros((N+1, S, A))\n",
    "        for n in range(N+1):\n",
    "            Qs_U = Qs_U.at[n].set(Qs[n, idx[n]])\n",
    "        dsU = ds[idx]\n",
    "        chex.assert_shape(dsU, (N+1, S))\n",
    "\n",
    "        worst_vio_idx = jnp.argmax(Js.max(axis=-1) - b0_threshes)\n",
    "        QU, dU = Qs_U[worst_vio_idx], dsU[worst_vio_idx]\n",
    "        grad = dU.reshape(-1, 1) * QU\n",
    "        policy = proj_to_Pi(policy - lr * grad)\n",
    "        return policy\n",
    "    \n",
    "    policy = jax.lax.fori_loop(0, num_iter, loop_fn, init_policy)\n",
    "    return policy\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update_outer_EF(args, init_k: int, end_k: int, rcmdp: RCMDP, inner_iter: int=1000, inner_lr: float = 0.001):\n",
    "    \"\"\"Update the threshold variable (end_k - init_k) times.\n",
    "    See Algorithm 2 in the paper.\n",
    "\n",
    "    Args:\n",
    "        init_args: These arguments will be passed to the foriloop of jax. See InitEFArgs defined above.\n",
    "        init_k (int): initial update index\n",
    "        end_k (int): end of update index\n",
    "\n",
    "    Returns:\n",
    "        args: Computed arguments. See InitEFArgs defined above.\n",
    "    \"\"\"\n",
    "    def body_fn(k, args):\n",
    "        res_J0U_list, vios_list, policy, i, j = args\n",
    "        b0 = (i + j) / 2\n",
    "        policy = solve_inner_EF(b0, rcmdp, policy, inner_iter, inner_lr)\n",
    "        Qs = compute_policy_Q(rcmdp.discount, policy, rcmdp.costs, rcmdp.U)  # N+1 x |U| x S x A\n",
    "        Js = jnp.sum((Qs * policy.reshape(1, 1, S, A)).sum(axis=-1) * rcmdp.init_dist.reshape(1, 1, S), axis=-1).max(axis=-1)\n",
    "        b0_threshes = jnp.hstack([jnp.array([b0,]), rcmdp.threshes])\n",
    "        Phi = jnp.max(Js - b0_threshes)\n",
    "\n",
    "        i = jax.lax.cond(Phi > 0, lambda: b0, lambda: i)\n",
    "        j = jax.lax.cond(Phi <= 0, lambda: b0, lambda: j)\n",
    "\n",
    "        res_J0U_list = res_J0U_list.at[k].set(Js[0])\n",
    "        vio = Js[1:] - rcmdp.threshes\n",
    "        vios_list = vios_list.at[k].set(vio.max())\n",
    "        return res_J0U_list, vios_list, policy, i, j\n",
    "\n",
    "    args = jax.lax.fori_loop(init_k, end_k, body_fn, args)\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.pmap, in_axes=(0, None, None, 0, 0))\n",
    "def update_Args(seed, init_k, end_k, LagArgs, EFArgs):\n",
    "    rcmdp = create_rcmdp(seed)\n",
    "    LagArgs = update_outer_Lagrange(LagArgs, init_k, end_k, rcmdp)\n",
    "    EFArgs = update_outer_EF(EFArgs, init_k, end_k, rcmdp)\n",
    "\n",
    "    uniform_policy = jnp.ones((S, A)) / A\n",
    "    Qs = compute_policy_Q(rcmdp.discount, uniform_policy, rcmdp.costs, rcmdp.U)  # N+1 x |U| x S x A\n",
    "    Js = jnp.sum((Qs * uniform_policy.reshape(1, 1, S, A)).sum(axis=-1) * rcmdp.init_dist.reshape(1, 1, S), axis=-1).max(axis=-1)\n",
    "    UJ, Uv = Js[0], jnp.max(Js[1:] - rcmdp.threshes)\n",
    "    return LagArgs, EFArgs, UJ, Uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [04:03<00:00, 24.32s/it]\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "UNROLL_ITER = 100\n",
    "seeds = jnp.arange(NUM_SEEDS)\n",
    "LagArgs, EFArgs = deepcopy(InitLagArgs), deepcopy(InitEFArgs)\n",
    "for i in tqdm(range(int(ITER_LENGTH / UNROLL_ITER))):\n",
    "    LagArgs, EFArgs, Uni_J0U_list, Uni_vio_list = update_Args(seeds, UNROLL_ITER * i, UNROLL_ITER * (i+1), LagArgs, EFArgs)\n",
    "Lag_J0U_list, Lag_vio_list, Lag_J0U_avg_list, Lag_vio_avg_list, *_ = LagArgs\n",
    "EF_J0U_list, EF_vio_list, *_ = EFArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"mathtext.fontset\"] = 'cm'\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "linestyle = itertools.cycle(('--', ':', '-', '-')) \n",
    "marker = itertools.cycle(('None', 'None', 'None', 'None')) \n",
    "alphas = itertools.cycle((1.0, 0.6, 1.0, 1.0)) \n",
    "Uni_J0U_list_rep = jnp.repeat(Uni_J0U_list.reshape(-1, 1), ITER_LENGTH, axis=1)\n",
    "Uni_vio_list_rep = jnp.repeat(Uni_vio_list.reshape(-1, 1), ITER_LENGTH, axis=1)\n",
    "J_baseval = Uni_J0U_list.reshape(-1, 1)\n",
    "\n",
    "algos =  {r\"Uniform policy ($\\pi_{\\mathrm{unif}}$)\": (Uni_J0U_list_rep - J_baseval, Uni_vio_list_rep),\n",
    "          \"LF-PGS\": (Lag_J0U_list - J_baseval, Lag_vio_list), \n",
    "          \"LF-PGS-avg\": (Lag_J0U_avg_list - J_baseval, Lag_vio_avg_list), \n",
    "          \"EpiRC-PGS (Ours)\": (EF_J0U_list - J_baseval, EF_vio_list), \n",
    "}\n",
    "\n",
    "performance_label = r\"Objective Return\"\n",
    "violation_label = r\"Constraint Violation\"\n",
    "iteration_label = r\"Outer Iteration $k$\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(11, 4))\n",
    "sns.set_theme(font_scale=1.2)\n",
    "axes = []\n",
    "ticksize = 8\n",
    "\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    ax = fig.add_subplot(121)\n",
    "    for name, J_v in algos.items():\n",
    "        J_list, v_list = J_v\n",
    "        dfs = []\n",
    "        for J in J_list:\n",
    "            dfs.append(pd.DataFrame({performance_label: J, iteration_label: np.arange(len(J))}))\n",
    "        df = pd.concat(dfs)\n",
    "        sns.lineplot(df, x=iteration_label, y=performance_label,  errorbar=None, label=name, linestyle=next(linestyle), marker=next(marker), markevery=10, markersize=3, markeredgecolor=\"tab:red\", alpha=next(alphas), legend=False)\n",
    "    plt.ylim(None, 2)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.yscale(\"symlog\")\n",
    "    plt.ylabel(\"Relative Objective Return\" \"\\n\" \"(Lower is better)\")\n",
    "    ax.tick_params(axis='both', which='major', labelsize=ticksize)\n",
    "\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    ax = fig.add_subplot(122)\n",
    "    ax.fill_between(x=[0, 1000], y1=0, y2=20, color=\"yellow\", alpha=0.2)\n",
    "    for name, J_v in algos.items():\n",
    "        J_list, v_list = J_v\n",
    "        dfs = []\n",
    "        for v in v_list:\n",
    "            dfs.append(pd.DataFrame({violation_label: v, iteration_label: np.arange(len(v))}))\n",
    "        df = pd.concat(dfs)\n",
    "        sns.lineplot(df, x=iteration_label, y=violation_label,  errorbar=None, label=name, linestyle=next(linestyle), marker=next(marker), markevery=10, markersize=3, markeredgecolor=\"tab:red\", alpha=next(alphas), legend=False)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.yscale(\"symlog\")\n",
    "    plt.ylim(-10, 10)\n",
    "    plt.ylabel(\"Constraint Violation\" \"\\n\" r\"(Yellow area violates constraint)\")\n",
    "    ax.tick_params(axis='both', which='major', labelsize=ticksize)\n",
    "    fig.tight_layout()\n",
    "\n",
    "algo_names = list(algos.keys())\n",
    "handles = [None] * len(algo_names)\n",
    "handle, label = ax.get_legend_handles_labels()\n",
    "for h, name in zip(handle, label):\n",
    "    handles[algo_names.index(name)] = h\n",
    "\n",
    "lgd = fig.legend(handles, algo_names, loc=\"upper center\", bbox_to_anchor=(0.5, 1.1), ncol=len(algo_names))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig(f\"double-loop.pdf\", bbox_extra_artists=(lgd, ), bbox_inches=\"tight\")\n",
    "plt.savefig(f\"double-loop.png\", bbox_extra_artists=(lgd, ), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
